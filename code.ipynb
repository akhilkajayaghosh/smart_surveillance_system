{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\akhil\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[INFO] Starting audio detection thread...\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "[2025-01-16 20:33:40.985047] Detected audio anomaly: Human Conversation (Confidence: 0.96)\n",
      "[INFO] Detected 'Human Conversation'. Audio will not be recorded.\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "[2025-01-16 20:33:51.327724] Detected audio anomaly: Human Conversation (Confidence: 0.96)\n",
      "[INFO] Detected 'Human Conversation'. Audio will not be recorded.\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "[2025-01-16 20:33:56.488523] Detected audio anomaly: Human Conversation (Confidence: 0.76)\n",
      "[INFO] Detected 'Human Conversation'. Audio will not be recorded.\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "[2025-01-16 20:34:27.491613] Detected audio anomaly: Human Conversation (Confidence: 0.71)\n",
      "[INFO] Detected 'Human Conversation'. Audio will not be recorded.\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "[2025-01-16 20:34:54.333299] Detected audio anomaly: Human Conversation (Confidence: 0.77)\n",
      "[INFO] Detected 'Human Conversation'. Audio will not be recorded.\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "[INFO] Recording audio...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "[INFO] Recording audio...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import threading\n",
    "import cv2\n",
    "from scipy.io.wavfile import write\n",
    "from scipy.signal import butter, lfilter\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime\n",
    "from deepface import DeepFace\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# === Model Loading ===\n",
    "yolo_model_path = 'yolo_v2.pt'  # YOLO model path\n",
    "\n",
    "# Load models\n",
    "audio_model = tf.keras.models.load_model(\"best_model1.keras\")\n",
    "yamnet_model = hub.load('https://www.kaggle.com/models/google/yamnet/tensorFlow2/yamnet/1?tfhub-redirect=true')\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "\n",
    "# Ensure required directories exist\n",
    "os.makedirs(\"anomalous_videos\", exist_ok=True)\n",
    "os.makedirs(\"anomalous_audio\", exist_ok=True)\n",
    "os.makedirs(\"unknown_faces\", exist_ok=True)\n",
    "\n",
    "# Index-to-label mapping for YAMNet\n",
    "index_to_label = {\n",
    "    0: \"Emergency_alert_sound\",\n",
    "    1: \"Explosions\",\n",
    "    2: \"Gunshots\",\n",
    "    3: \"Human screams\",\n",
    "    4: \"Bottles breaking\",\n",
    "    5: \"Dog bark\",\n",
    "    6: \"Human Conversation\",\n",
    "    # Add more labels as per your trained model\n",
    "}\n",
    "\n",
    "# Noise reduction using a Butterworth filter\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut=300.0, highcut=3400.0, fs=16000, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Preprocess and generate YAMNet embeddings\n",
    "def preprocess_audio(audio_waveform, target_length=16000 * 5):\n",
    "    # Apply noise reduction\n",
    "    audio_waveform = bandpass_filter(audio_waveform)\n",
    "\n",
    "    # Pad or truncate the waveform to the target length\n",
    "    if len(audio_waveform) < target_length:\n",
    "        audio_waveform = np.pad(audio_waveform, (0, target_length - len(audio_waveform)))\n",
    "    else:\n",
    "        audio_waveform = audio_waveform[:target_length]\n",
    "\n",
    "    # Normalize between -1 and 1\n",
    "    audio_waveform = audio_waveform.astype(np.float32)\n",
    "    audio_waveform /= np.max(np.abs(audio_waveform))\n",
    "    return audio_waveform\n",
    "\n",
    "def predict_audio_from_stream(audio_data, sr, confidence_threshold=0.7):\n",
    "    processed_audio = preprocess_audio(audio_data)\n",
    "    _, yamnet_embeddings, _ = yamnet_model(processed_audio)\n",
    "    avg_embedding = tf.reduce_mean(yamnet_embeddings, axis=0).numpy().reshape(1, -1)\n",
    "\n",
    "    prediction = audio_model.predict(avg_embedding)\n",
    "    predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "    confidence = prediction[0][predicted_class_index]\n",
    "\n",
    "    if confidence >= confidence_threshold:\n",
    "        predicted_class_name = index_to_label.get(predicted_class_index, \"Unknown\")\n",
    "        return predicted_class_name, confidence\n",
    "    return \"Unknown\", confidence\n",
    "\n",
    "# Audio capturing and processing thread\n",
    "def process_audio_stream(duration=5, sample_rate=16000):\n",
    "    print(\"[INFO] Starting audio detection thread...\")\n",
    "    while True:\n",
    "        print(\"[INFO] Recording audio...\")\n",
    "        audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')\n",
    "        sd.wait()  # Wait until the recording is finished\n",
    "        audio_data = audio_data.flatten()\n",
    "\n",
    "        predicted_class, confidence = predict_audio_from_stream(audio_data, sample_rate)\n",
    "\n",
    "        if predicted_class != \"Unknown\":\n",
    "            print(f\"[{datetime.now()}] Detected audio anomaly: {predicted_class} (Confidence: {confidence:.2f})\")\n",
    "            \n",
    "            # Skip recording if the detected audio is \"Human Conversation\"\n",
    "            if predicted_class == \"Human Conversation\":\n",
    "                print(\"[INFO] Detected 'Human Conversation'. Audio will not be recorded.\")\n",
    "                continue\n",
    "            \n",
    "            audio_filename = f\"anomalous_audio/{predicted_class}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav\"\n",
    "            write(audio_filename, sample_rate, (audio_data * 32767).astype(np.int16))\n",
    "            print(f\"[INFO] Audio saved to: {audio_filename}\")\n",
    "\n",
    "\n",
    "# Face recognition setup\n",
    "known_faces_dir = \"known_faces\"\n",
    "os.makedirs(known_faces_dir, exist_ok=True)\n",
    "known_encodings = []\n",
    "known_names = []\n",
    "\n",
    "# Load known faces\n",
    "def load_known_faces():\n",
    "    for name in os.listdir(known_faces_dir):\n",
    "        person_dir = os.path.join(known_faces_dir, name)\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(person_dir):\n",
    "            file_path = os.path.join(person_dir, file)\n",
    "            try:\n",
    "                encoding = DeepFace.represent(img_path=file_path, model_name='Facenet')[0]['embedding']\n",
    "                known_encodings.append(encoding)\n",
    "                known_names.append(name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading face {file_path}: {e}\")\n",
    "\n",
    "load_known_faces()\n",
    "\n",
    "def recognize_faces(frame):\n",
    "    try:\n",
    "        detections = DeepFace.find(img_path=frame, db_path=known_faces_dir, model_name='Facenet', enforce_detection=False)\n",
    "        if detections and not detections[0].empty:  # Check if the list is non-empty and DataFrame is not empty\n",
    "            return detections[0].iloc[0]['identity']  # Return the first match\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during face recognition: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def is_face_saved(face_encoding, threshold=0.6):\n",
    "    \"\"\"Check if a face encoding is already saved by comparing it to existing encodings.\"\"\"\n",
    "    for known_encoding in known_encodings:\n",
    "        # Compute cosine similarity between the new face and the known faces\n",
    "        similarity = cosine_similarity([face_encoding], [known_encoding])[0][0]\n",
    "        if similarity > threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Save the unknown face to the folder\n",
    "def save_unknown_face(frame):\n",
    "    try:\n",
    "        # Extract face encoding\n",
    "        encoding = DeepFace.represent(img_path=frame, model_name='Facenet')[0]['embedding']\n",
    "\n",
    "        # Check if face already exists in the folder\n",
    "        if not is_face_saved(encoding):\n",
    "            face_filename = f\"unknown_faces/unknown_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg\"\n",
    "            cv2.imwrite(face_filename, frame)\n",
    "            print(f\"[INFO] Unknown face saved to: {face_filename}\")\n",
    "            known_encodings.append(encoding)  # Add new encoding to the list\n",
    "        else:\n",
    "            print(f\"[INFO] Duplicate unknown face detected, not saving.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving unknown face: {e}\")\n",
    "\n",
    "# Video threat detection\n",
    "def process_video_stream(video_source=0):\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video stream\")\n",
    "        return\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "    out = None\n",
    "    recording = False\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Warning: Failed to capture frame, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Perform face recognition\n",
    "        face_result = recognize_faces(frame)\n",
    "        if face_result == \"Unknown\":\n",
    "            print(f\"[{datetime.now()}] Unknown face detected!\")\n",
    "            save_unknown_face(frame)\n",
    "\n",
    "        # Perform object detection\n",
    "        # Perform object detection\n",
    "        results = yolo_model.predict(frame, conf=0.5, verbose=False)\n",
    "\n",
    "        anomalies_detected = False\n",
    "        if results[0].boxes:\n",
    "            for box in results[0].boxes:\n",
    "                # Extract bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].numpy())  # Convert to integers\n",
    "                cls_id = int(box.cls)\n",
    "                confidence = box.conf.item()\n",
    "                label = yolo_model.names[cls_id]\n",
    "\n",
    "                # Draw bounding box and label on the frame\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Red box\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"{label} ({confidence:.2f})\",\n",
    "                    (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0, 255, 255),\n",
    "                    2,\n",
    "                )  # Yellow label\n",
    "\n",
    "                print(f\"[{datetime.now()}] Detected: {label} (Confidence: {confidence:.2f})\")\n",
    "                if label in [\"violence\", \"weaponized\"]:\n",
    "                    anomalies_detected = True\n",
    "\n",
    "                    # Start recording if not already recording\n",
    "                    if not recording:\n",
    "                        video_filename = f\"anomalous_videos/{label}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4\"\n",
    "                        out = cv2.VideoWriter(video_filename, fourcc, fps, (frame_width, frame_height))\n",
    "                        recording = True\n",
    "                        print(f\"[{datetime.now()}] Recording started: {video_filename}\")\n",
    "\n",
    "            # Save the annotated frame to the video file if recording\n",
    "            if recording and out:\n",
    "                out.write(frame)\n",
    "\n",
    "        # Stop recording if no anomalies are detected\n",
    "        if not anomalies_detected and recording:\n",
    "            print(f\"[{datetime.now()}] Anomaly ended. Stopping recording.\")\n",
    "            recording = False\n",
    "            if out:\n",
    "                out.release()\n",
    "                out = None\n",
    "\n",
    "\n",
    "                if not anomalies_detected and recording:\n",
    "                    print(f\"[{datetime.now()}] Anomaly ended. Stopping recording.\")\n",
    "                    recording = False\n",
    "                    if out:\n",
    "                        out.release()\n",
    "                        out = None\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Exit requested. Stopping program.\")\n",
    "            break\n",
    "\n",
    "    if recording and out:\n",
    "        out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Unified system\n",
    "def unified_system(video_source=0):\n",
    "    # video_thread = threading.Thread(target=process_video_stream, args=(video_source,))\n",
    "    audio_thread = threading.Thread(target=process_audio_stream)\n",
    "\n",
    "    # video_thread.start()\n",
    "    audio_thread.start()\n",
    "\n",
    "    # video_thread.join()\n",
    "    audio_thread.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unified_system()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
